\chapter{Introduction}

\section{Motivation}
Many scientific and business computing applications must work on large data sets 
naturally structured in regular, multidimensional collections.
In order for these applications to achieve good performance, 
it is often the case that programmers must exploit any and all potential concurrency in the application 
through different approaches to parallel programming.
One form of potential concurrency frequently encountered when programs operate on large collections of data 
is \textit{data parallelism}, which is when programs update elements or subsections of collections in parallel.
Many tools have already been developed to help programmers exploit data parallelism\cite{dph}\cite{openmp}
easily and safely (i.e., with reduced risk of race conditions or program crashes). 
However, many of these tools may not be as helpful 
for exploiting data parallelism on regular collections as they could be, 
and thus make it more difficult for programs to achieve good performance, for two reasons.

First, many programming languages implement regular collections as nested collections;
e.g. a 2-dimensional, 2 by 3 array (or rank 2 array with shape 2 3) of integers would be implemented as 
an array of 2 arrays, with each of these containing 3 integers. 
However, in such languages irregular collections are also usually implemented as nested collections, 
with each of the sub-collections possibly containing a different number of elements from each other.
Since both regular and irregular collections are implemented the same way,
it can be difficult for programmers to view, manipulate, and verify 
any important properties of the structure of regular collections.
For example, if a function requires that one or more of its arguments is a regular collection, 
programmers must either write code to ensure this requirement is met,
increasing the difficulty of writing what could be an already complex program, 
or risk run-time indexing errors and program crashes.

Alternatively, regular collections can be implemented as vectors which have an associated shape vector. 
These shape-associated collections make the structure of a regular collection 
a field which can be viewed and manipulated and allow the programmer to have 
greater transparency and control when writing programs to operate on them.
Unfortunately, most statically-typed languages do not have type systems advanced enough 
to capture all of the structural information of shape-associated collections statically, 
even though this is theoretically possible. % TODO cite dependent types (xi probably)
However, some languages are able to capture some of this structure statically\cite{boost}\cite{sac}\cite{dph}, 
enabling programmers to reduce or eliminate the need for manual verification, 
such as the fact that the arguments to a function are in fact regular collections,
or that some collection satisfies a function's lower and/or upper bounds for rank.
Even when these properties of regular collections cannot be captured statically, 
making this information more transparent at run time 
should allow programmers to more easily verify that the required properties hold.

Second, in most imperative languages with regular collections, 
applying functions to elements or sub-collections of a collection means 
writing the function call within nested looping structures, typically a \textit{for loop}
Using this method to apply functions on these collections usually means that 
the number of dimensions (also called \textit{data rank}) of the collection 
determines the number of loops required to do a specific operation.
To put it another way, if an existing function that operates on regular collections 
needs to be extended to a collection of higher rank, 
or if the function only operates on scalar values but needs to be extended to some regular collection, 
the programmer usually must wrap the function in nested for loops.
Without abstraction in the language or library to deal with the general cases of such extensions, 
this activity is tedious in the trivial cases, adding unnecessary overhead to the programming process,
and prone to error because programmers must instead write boilerplate code in order to accomplish it.
Moreover, in some cases these extensions are data parallel, 
and can be done automatically and automatically exploit potential concurrency.
However, using nested for loops obfuscates,
and requires the programer make changes to existing code in order to exploit, this inherent concurrency. 
It seems clear that it would be better for programmers to express the potential concurrency directly, 
rather than expressing it as sequential for loop and either hoping a compiler can reverse this transformation 
or writing additional code to exploit the concurrency.

Functional programming languages, in contrast to imperative programming languages, 
allow programmers to view and write programs at a higher level of abstraction.
When dealing with collections, 
this higher level of abstraction usually means that related types of operations can be expressed as \textit{higher-order} functions 
like \textit{map} and \textit{reduce} that take functions as their arguments and return functions that operate on collections.
These higher-order functions can allow programmers to more easily understand what operations are being done to regular collections 
than if the same operations were done imperatively using for loops 
because they capture the essence of what the operations \textit{are}, more than how the operations are \textit{implemented}.
As a consequence, they also can allow programmers 
to see more easily where there is inherent data parallelism in the algorithm, 
such as all calls to \textit{map}, or calls to \textit{reduce} with associative operations, on large collections.
However, since these higher-order functions are usually designed to return functions that operate on only one dimension at a time, 
they too must be nested in order to extend existing functions to collections of higher rank, 
with the same, though usually reduced, problem of tediousness and boilerplate code mentioned above.

\textit{Function rank}, first introduced by K. Iverson in 1978\cite{opandfunc} 
and implemented in the programming language J, 
is a functional abstraction that extends the notion of data rank to functions. 
In functional languages where the idea of function rank is formalized, 
extending existing functions to regular collections of higher dimension 
can be expressed as the application of a higher-ordered function, 
called in J and in this paper the \textit{rank operator}\cite{jvocab}. 
Expressing these extensions as a higher-order function 
makes it easier for a programmer to make them safely, quickly, and at a higher level of abstraction.
In some cases, these extensions are so trivial that they can be done automatically\cite{jvocab} \cite{rankanduni}, 
meaning the programmer need not modify the code at all. 
Furthermore, since multiple applications of the rank operator 
are equivalent to the cases where nested loops or nested higher-order function applications are inherently data parallel, 
it too is inherently data parallel.
Consequently, languages with both formalized function rank and a rank-operator allow the programmer to 
exploit the inherent data parallelism of extending existing operations to collections of higher rank 
safely, quickly, and in some cases automatically.

However, currently the languages that meet these criteria, 
such as J, Sharp APL, and some others from the APL language family, 
are not in common use. % TODO cite lack of parallelism
One often-cited reason for this is that these languages are difficult to read, % TODO cite
because, in order to use them effectively, a programmer must memorize 
dozens of 1 or 2 character functions each with different, sometimes unrelated use cases 
depending on whether the function takes one or two arguments\cite{jvocab}\cite{dapl}. 
These language design choices, however, 
are not required in order for a language to support function rank.
It seems that a proof-of-concept may be needed 
to demonstrate that both the notions of function rank
and associating collections with their shape
are still very helpful for exploiting data parallelism on regular collections 
when available in a more modern, more popularly supported language.

The rest of the paper is organized as follows:
\begin{itemize} 
	\item The remaining sections of this chapter give the design plan (\ref{desp}) and implementation (\ref{imp}) of our research. 
	Unfortunately, the latter did not quite fulfill the full scope of the former, 
	so both a description of what was planned and what was actually accomplished within the time constraints are given.
	\item Chapter \ref{back} gives the necessary background information for understanding this work, 
	including a brief description of function rank and how it is equivalent to nested loops or nested calls to \textit{map},
	a discussion of relevant parallel design patterns for the example problem set, 
	and a literature review of other attempts to solve the same or similar problems
	\item Chapter \ref{paraop} proposes two new operators for future parallel implementations of the J programming language 
		and illustrates how these operators might be used
    \item Chapter \ref{probs} gives the listing of our selection of example problems, giving briefly for each a description, a discussion of the relevant parallel design patterns for exploiting concurrency, and a description of how possibly to extend the problem to higher dimensions.
    \item Chapter \ref{res} compares example solutions to the problems listed in Chapter \ref{probs} 
		in J, Scala with Parallel-J, and C with OpenMP, 
		and discusses the relative level of abstraction, scalability and performance of each solution. %(CJ) TODO remove falsehoods - probably won't compare with all
    \item Chapter \ref{conc} presents our conclusions of this research and discusses future work. 
\end{itemize}

\section{Design Plan}
\label{desp}
The goal of this research was
to implement a parallel subset of the programming language J, called \textit{Parallel-J}.
Based on the documentation of J's current implementation\cite{ioj}, it was determined that this language subset would require:
\begin{itemize}
	\item a J language lexer and parser
	\item limited memory management
	\item a look-up table for predefined and user functions
	\item a read-evaluate-print-loop (REPL), and
	\item a subset of the J primitives, including: 
	\begin{itemize}
		\item global variable assignment
		\item most array operations (creating, indexing, restructuring)
		\item all of the basic arithmetic and logical operations
		\item a limited selection of higher-ordered functions (composition, conditional, reduction)
		\item J's function composition rules for sequences of functions (\textit{trains}), and
		\item the rank operator and function rank
	\end{itemize}
\end{itemize}

Excluded from the design of the parallel language subset the following language features: 
\begin{itemize}
	\item namespaces (\textit{locales}) and local variable assignment
	\item all existing environment-altering functions (the \textit{foreign} operator, spelled \ttfamily!:\normalfont)
	\item explicit scripts with imperative-style execution and control structure, and
	\item the J primitives which calculate complex algorithms (prime factorization, derivatives of polynomial equations, etc.)
\end{itemize}

These choices were motivated by the desire to present solutions to our sample problems in as simple and ``idiomatic'' J as possible.
This would allow us to directly address the involved computations, 
as well as the advantages to exploiting data parallelism when approaching these problems with function rank, 
without getting distracted with a discussion of J's more advanced or nuanced language features.

Also to be included was a proposal and implementation of two new operators (parallel rank and parallel insert), 
and an extension to the existing \textit{foreign} operator to allow for changing the parallel environment. 
The former would allow for the equivalent of parallel map and reduce operations at the specified rank(s). 
The latter would allow the programmer to set environmental parameters for parallelizing code, 
for example specifying the number of threads to use in executing a piece of code, or setting thread scheduling schemes.
Although not entirely related to function rank, such environmental controls as would be included in the parallel environment 
are to be expected from any serious parallel computing language or library.

The Scala programming language\cite{scala} 
was chosen to implement this parallel subset of J.
Scala has many features that make it desirable for this task, including: 
\begin{enumerate} 
	\item support for multiple programming paradigms, such as
	\begin{itemize}
		\item imperative, for the tasks of memory management and object creation,
		\item object oriented, for more structured encapsulation of the rank associated with each function 
			and data associated with arrays, and
		\item functional, to more easily capture J's functional paradigm in the implementation and thus facilitate development
	\end{itemize}
	\item a feature-rich library for collections, including 
	\begin{itemize}
		\item support for many higher-order operations such as \textit{map} and \textit{reduce}\cite{scala28col}, 
			common to many solutions to data parallel problems, and
		\item a library for parallel collections that exploits the concurrency inherently data parallel operations\cite{pc},
			which, being similar to the use-cases in this research, were anticipated to make parallelizing the implementation relatively easy.
	\end{itemize}
\end{enumerate}

The performance of solutions written in Parallel-J to a suite of problems would be compared to
performance of the same solutions written in C with OpenMP, J, and Scala.
Also given for each problem would be 
a discussion of the relative level of abstraction, scalability to similar problems of higher dimension, and performance of each. 
It was not expected that Parallel-J would compare favorably to the raw performance of C or the current implementation of J; 
the hoped for result was to show that the performance Parallel-J scales relative to the number of threads, 
showing that this implementation does effectively exploit potential concurrency.

\section{Implementation}
\label{imp}
Unfortunately, during the development of this research it became clear 
that the given time constraints would not allow for such an ambitious project.
This has lead to a change of focus in the research. 
Rather than finishing a parallel subset of J, the J system libraries developed 
are instead used as a prototype for a Scala regular collections library which supports function rank.
Taking the same set of parallel problems, 
this research shows both how this library allows for uniformly extending these problems into higher dimensions and 
how with future work it would be able to achieve good performance 
by automatically parallelizing solutions to problems in the given and in higher dimensions.
Results are also compared with solutions written in C with OpenMP, 
along with a discussion of the relative level of abstraction, scalability, and performance of each.

The proposal for the \textit{parallel operator} is still given but it unfortunately could not be implemented in the time available.

Finally, some work completed with the idea of implementing a subset of J 
did not contribute to developing a regular collections library in Scala that supports function rank.
Most of this work consisted of implementing a lexer for the J programming language, which is listed in Appendix \ref{jlex}
